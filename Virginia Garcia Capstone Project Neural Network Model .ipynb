{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677e9f67",
   "metadata": {},
   "source": [
    "# Neural Network Model for ICD-10 Code Prediction\n",
    "\n",
    "## Introduction\n",
    "This neural network model is designed to predict ICD-10 codes based on patient notes. The model mimics the structure and functioning of the human brain, consisting of layers of interconnected nodes (neurons). These nodes are adjusted during training to minimize prediction errors and improve the model's accuracy.\n",
    "\n",
    "## Libraries Used and Their Purpose\n",
    "The model utilizes several key Python libraries to handle various tasks:\n",
    "\n",
    "- **TensorFlow/Keras**: These libraries are used for building and training the neural network. TensorFlow provides a robust platform for deep learning, and Keras offers a high-level API for easy model creation.\n",
    "- **Pandas**: This library is used for data manipulation and analysis. It helps in loading and preparing the dataset.\n",
    "- **NLTK (Natural Language Toolkit)**: NLTK is used for text preprocessing, including tokenization, removing stop words, and lemmatization, which are essential steps in preparing the text data for modeling.\n",
    "- **Scikit-learn**: This library is used for additional preprocessing steps like vectorization of text data using TF-IDF and encoding the ICD-10 labels. It also provides functions for evaluating the model's performance.\n",
    "- **Imbalanced-learn**: This library is used to handle class imbalance in the dataset. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) are employed to ensure that the model performs well even with underrepresented classes.\n",
    "- **Pickle**: This library is used for serializing and deserializing Python objects. It allows saving and loading the trained model, vectorizer, and label encoder without retraining the model each time.\n",
    "- **re (Regular Expressions)**: This module is used for text preprocessing. It helps in cleaning the text data by removing unwanted characters, numbers, and punctuations based on specified patterns.\n",
    "\n",
    "## How the Neural Network Works\n",
    "Neural networks are a type of artificial intelligence modeled after the human brain. They consist of layers of interconnected nodes (neurons) that process data in complex ways to learn patterns and make predictions.\n",
    "\n",
    "### Layers of the Neural Network\n",
    "1. **Input Layer**: This is the first layer of the network and it accepts the processed text data (patient notes) as input. The text data is converted into numerical form so that the network can work with it.\n",
    "2. **Hidden Layers**: These are the intermediate layers between the input and output layers. Each hidden layer consists of multiple neurons. These neurons apply mathematical operations to the input data and learn complex patterns by adjusting their internal parameters (weights) during training. The activation function (ReLU in this case) helps the network learn non-linear relationships in the data.\n",
    "3. **Output Layer**: This is the final layer of the network. It produces a probability distribution over all possible ICD-10 codes and outputs the most likely code for a given patient note. The softmax activation function is used here to convert the raw output into probabilities that sum up to 1.\n",
    "\n",
    "### Training the Neural Network\n",
    "- **Forward Propagation**: The input data is passed through the network layer by layer. At each layer, the neurons perform calculations and transform the data.\n",
    "- **Loss Calculation**: The network's prediction is compared to the actual ICD-10 code, and the difference (error) is calculated using a loss function.\n",
    "- **Backpropagation**: The network adjusts its weights to minimize the error by propagating the error backward through the network and updating the weights.\n",
    "- **Iteration**: This process is repeated for multiple iterations (epochs) until the network learns to make accurate predictions.\n",
    "\n",
    "## Model Summary and Results\n",
    "### Model Architecture\n",
    "The neural network model is designed to predict ICD-10 codes based on patient notes. The architecture consists of:\n",
    "- **Input Layer**: Dense layer with 512 neurons, ReLU activation, and a dropout rate of 0.5 to prevent overfitting.\n",
    "- **Hidden Layer**: Dense layer with 256 neurons, ReLU activation, and a dropout rate of 0.5.\n",
    "- **Output Layer**: Dense layer with neurons equal to the number of unique ICD-10 codes, using softmax activation to output a probability distribution.\n",
    "\n",
    "### Model Performance\n",
    "The model has undergone several iterations and training with different datasets and parameters. Key performance metrics include:\n",
    "- **Accuracy**: 72.67%\n",
    "- **Precision**: 73.37%\n",
    "- **Recall**: 72.67%\n",
    "- **F1-Score**: 72.29%\n",
    "\n",
    "### Limitations and Suggestions for Improvement\n",
    "#### Limitations\n",
    "- **Class Imbalance**: Certain ICD-10 codes with fewer samples exhibited low recall and precision, highlighting the challenge of accurately predicting less frequent classes.\n",
    "- **Overfitting**: The model showed signs of overfitting, as indicated by the increasing validation loss after the second epoch.\n",
    "\n",
    "#### Suggestions for Improvement\n",
    "- **Increase Training Epochs with Early Stopping**: Increasing the number of epochs while implementing early stopping could help capture more details without overfitting.\n",
    "- **Class Weight Adjustment**: Adjusting class weights during training to give more importance to underrepresented classes can improve recall for these classes.\n",
    "- **Data Augmentation**: Augmenting data for less frequent classes can help the model learn better representations for these classes.\n",
    "- **Hyperparameter Tuning**: Tuning hyperparameters such as learning rate, batch size, and dropout rates can help find the optimal configuration for the model.\n",
    "- **Advanced Oversampling Techniques**: Employing techniques like SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN (Adaptive Synthetic) could better handle class imbalance.\n",
    "\n",
    "## Input/Output Description\n",
    "### Input\n",
    "- **Patient Notes**: The input to the model consists of text data from patient notes which are preprocessed and vectorized.\n",
    "\n",
    "### Output\n",
    "- **ICD-10 Code**: The output is a predicted ICD-10 code which represents a specific diagnosis.\n",
    "\n",
    "## Example Chart Notes\n",
    "Here are some example chart notes you can use to test the model:\n",
    "\n",
    "1. **Example 1**: Patient complains of severe lower back pain that has persisted for several weeks. Pain radiates down the left leg. MRI indicates a herniated disc at the L4-L5 level.\n",
    "2. **Example 2**: Patient presents with uncontrolled diabetes mellitus. Blood sugar levels have been consistently high despite adherence to prescribed medication and dietary restrictions. Complains of frequent urination, increased thirst, and fatigue.\n",
    "3. **Example 3**: Patient reports chest pain radiating to the left arm and jaw, shortness of breath, and sweating. Symptoms began suddenly while exercising. History of hypertension and high cholesterol. ECG shows ST elevation.\n",
    "\n",
    "Feel free to run the model with these example chart notes to see how it predicts the ICD-10 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2430e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ricky\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\ricky\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNLTK (Natural Language Toolkit) is a Python library that provides tools for handling human language data (text). \\nIt supports a variety of NLP tasks such as tokenization, parsing, and tagging, and includes resources for \\nbuilding machine learning-based language processing models.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers nltk\n",
    "\n",
    "\"\"\"\n",
    "NLTK (Natural Language Toolkit) is a Python library that provides tools for handling human language data (text). \n",
    "It supports a variety of NLP tasks such as tokenization, parsing, and tagging, and includes resources for \n",
    "building machine learning-based language processing models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d311d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ricky\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in c:\\users\\ricky\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: rich in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ricky\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTensorFlow is an open-source library developed by Google for deep learning and machine learning tasks. It provides \\na flexible and comprehensive ecosystem of tools, libraries, and community resources that lets researchers push \\nthe state-of-the-art in ML, and developers easily build and deploy ML-powered applications.\\n\\nKeras is a high-level API for building and training deep learning models. It runs on top of TensorFlow and allows \\nfor easy and fast prototyping by providing simple and user-friendly methods for creating complex neural network \\narchitectures.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tensorflow keras\n",
    "\n",
    "\"\"\"\n",
    "TensorFlow is an open-source library developed by Google for deep learning and machine learning tasks. It provides \n",
    "a flexible and comprehensive ecosystem of tools, libraries, and community resources that lets researchers push \n",
    "the state-of-the-art in ML, and developers easily build and deploy ML-powered applications.\n",
    "\n",
    "Keras is a high-level API for building and training deep learning models. It runs on top of TensorFlow and allows \n",
    "for easy and fast prototyping by providing simple and user-friendly methods for creating complex neural network \n",
    "architectures.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33454be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ricky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ricky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\ricky\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m4046/4046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 68ms/step - accuracy: 0.6884 - loss: 1.6353 - val_accuracy: 0.7044 - val_loss: 2.7481\n",
      "Epoch 2/4\n",
      "\u001b[1m4046/4046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 66ms/step - accuracy: 0.9870 - loss: 0.0454 - val_accuracy: 0.7133 - val_loss: 3.3809\n",
      "Epoch 3/4\n",
      "\u001b[1m4046/4046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 66ms/step - accuracy: 0.9919 - loss: 0.0272 - val_accuracy: 0.7089 - val_loss: 3.7961\n",
      "Epoch 4/4\n",
      "\u001b[1m4046/4046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 68ms/step - accuracy: 0.9946 - loss: 0.0179 - val_accuracy: 0.7189 - val_loss: 4.1903\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7018 - loss: 4.7056\n",
      "Neural Network Model Accuracy: 71.89%\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricky\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 74.08%\n",
      "Recall: 71.89%\n",
      "F1-Score: 71.68%\n",
      "\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                     E08.00       0.00      0.00      0.00         1\n",
      "                     E08.11       0.00      0.00      0.00         0\n",
      "                     E08.21       0.00      0.00      0.00         1\n",
      "                     E08.29       0.00      0.00      0.00         1\n",
      "                   E08.3112       0.00      0.00      0.00         0\n",
      "                   E08.3192       1.00      1.00      1.00         1\n",
      "                   E08.3293       0.00      0.00      0.00         0\n",
      "                   E08.3312       0.00      0.00      0.00         2\n",
      "                   E08.3393       0.00      0.00      0.00         1\n",
      "                   E08.3399       0.00      0.00      0.00         0\n",
      "                   E08.3492       0.00      0.00      0.00         1\n",
      "                   E08.3493       0.00      0.00      0.00         0\n",
      "                   E08.3521       0.00      0.00      0.00         0\n",
      "                   E08.3529       0.00      0.00      0.00         1\n",
      "                   E08.3531       0.00      0.00      0.00         1\n",
      "                   E08.3532       0.00      0.00      0.00         2\n",
      "                   E08.3539       0.00      0.00      0.00         0\n",
      "                     E08.40       0.00      0.00      0.00         0\n",
      "                     E08.44       0.00      0.00      0.00         0\n",
      "                     E08.51       0.00      0.00      0.00         1\n",
      "                     E08.52       0.00      0.00      0.00         0\n",
      "                     E08.59       0.00      0.00      0.00         1\n",
      "                    E08.621       0.00      0.00      0.00         0\n",
      "                    E08.622       0.00      0.00      0.00         1\n",
      "                    E08.628       0.00      0.00      0.00         1\n",
      "                     E08.64       0.00      0.00      0.00         1\n",
      "                      E08.9       0.00      0.00      0.00         1\n",
      "                     E09.11       0.00      0.00      0.00         0\n",
      "                     E09.21       1.00      1.00      1.00         1\n",
      "                   E09.3213       0.00      0.00      0.00         1\n",
      "                   E09.3291       0.00      0.00      0.00         0\n",
      "                   E09.3311       0.50      1.00      0.67         1\n",
      "                   E09.3319       0.00      0.00      0.00         1\n",
      "                   E09.3413       0.00      0.00      0.00         0\n",
      "                   E09.3492       0.00      0.00      0.00         0\n",
      "                   E09.3493       0.00      0.00      0.00         1\n",
      "                   E09.3499       0.00      0.00      0.00         1\n",
      "                   E09.3511       0.00      0.00      0.00         2\n",
      "                   E09.3513       0.00      0.00      0.00         0\n",
      "                   E09.3522       0.00      0.00      0.00         1\n",
      "                   E09.3532       0.00      0.00      0.00         0\n",
      "                   E09.3541       0.00      0.00      0.00         0\n",
      "                   E09.3542       1.00      0.50      0.67         2\n",
      "                   E09.3559       1.00      1.00      1.00         1\n",
      "                   E09.3593       1.00      1.00      1.00         1\n",
      "                   E09.37X2       0.00      0.00      0.00         1\n",
      "                   E09.37X9       0.50      1.00      0.67         1\n",
      "                     E09.39       0.00      0.00      0.00         1\n",
      "                     E09.52       0.00      0.00      0.00         0\n",
      "                    E09.618       0.00      0.00      0.00         0\n",
      "                    E09.620       0.00      0.00      0.00         1\n",
      "                    E09.621       0.00      0.00      0.00         0\n",
      "                    E09.628       0.00      0.00      0.00         0\n",
      "                    E09.630       0.00      0.00      0.00         1\n",
      "                     E09.65       0.00      0.00      0.00         1\n",
      "                     E09.69       0.00      0.00      0.00         1\n",
      "                      E09.9       0.00      0.00      0.00         1\n",
      "                      E10.1       1.00      0.67      0.80         3\n",
      "                     E10.10       0.50      0.50      0.50         8\n",
      "                     E10.11       0.67      0.67      0.67        12\n",
      "                     E10.14       0.50      1.00      0.67         2\n",
      "                     E10.21       0.00      0.00      0.00         0\n",
      "                     E10.29       0.00      0.00      0.00         0\n",
      "                    E10.311       0.00      0.00      0.00         3\n",
      "                    E10.319       0.00      0.00      0.00         2\n",
      "                   E10.3291       0.00      0.00      0.00         0\n",
      "                   E10.3299       0.00      0.00      0.00         0\n",
      "                   E10.3311       0.00      0.00      0.00         0\n",
      "                   E10.3391       0.00      0.00      0.00         0\n",
      "                   E10.3392       1.00      1.00      1.00         1\n",
      "                   E10.3399       0.00      0.00      0.00         1\n",
      "                   E10.3411       0.00      0.00      0.00         1\n",
      "                    E10.351       0.25      0.50      0.33         2\n",
      "                   E10.3511       0.00      0.00      0.00         0\n",
      "                   E10.3519       0.00      0.00      0.00         0\n",
      "                   E10.3529       0.00      0.00      0.00         0\n",
      "                   E10.3539       0.00      0.00      0.00         2\n",
      "                   E10.3549       0.00      0.00      0.00         0\n",
      "                   E10.3551       0.00      0.00      0.00         1\n",
      "                    E10.359       0.00      0.00      0.00         1\n",
      "                   E10.3592       0.00      0.00      0.00         0\n",
      "                     E10.36       0.00      0.00      0.00         4\n",
      "                     E10.39       0.00      0.00      0.00         1\n",
      "                     E10.40       1.00      0.67      0.80         3\n",
      "                     E10.41       1.00      0.50      0.67         2\n",
      "                     E10.42       0.00      0.00      0.00         0\n",
      "                     E10.43       0.00      0.00      0.00         5\n",
      "                     E10.49       0.00      0.00      0.00         1\n",
      "                     E10.51       0.00      0.00      0.00         1\n",
      "                     E10.52       0.00      0.00      0.00         0\n",
      "                     E10.59       0.00      0.00      0.00         2\n",
      "                    E10.618       0.67      0.33      0.44         6\n",
      "                    E10.620       0.00      0.00      0.00         3\n",
      "                    E10.621       0.00      0.00      0.00         1\n",
      "                    E10.622       0.00      0.00      0.00         1\n",
      "                    E10.628       0.00      0.00      0.00         0\n",
      "                     E10.63       0.00      0.00      0.00         0\n",
      "                    E10.630       0.00      0.00      0.00         0\n",
      "                    E10.638       0.00      0.00      0.00         1\n",
      "                    E10.649       0.57      0.57      0.57         7\n",
      "                     E10.65       0.91      0.98      0.95        62\n",
      "                     E10.69       1.00      0.33      0.50         3\n",
      "                      E10.8       0.00      0.00      0.00         0\n",
      "                      E10.9       0.94      0.54      0.68        28\n",
      "                      E11.0       1.00      0.33      0.50         3\n",
      "                     E11.01       0.00      0.00      0.00         1\n",
      "                      E11.1       1.00      0.40      0.57         5\n",
      "                     E11.10       0.00      0.00      0.00         0\n",
      "                     E11.11       0.00      0.00      0.00         1\n",
      "                      E11.2       0.71      0.71      0.71         7\n",
      "                     E11.21       0.82      0.82      0.82        11\n",
      "                     E11.22       1.00      0.82      0.90        34\n",
      "              E11.22, N18.9       1.00      1.00      1.00         2\n",
      "                     E11.29       1.00      1.00      1.00         3\n",
      "                      E11.3       0.78      0.70      0.74        10\n",
      "                     E11.31       0.00      0.00      0.00         0\n",
      "                    E11.311       1.00      0.33      0.50         3\n",
      "                    E11.319       0.98      0.98      0.98        58\n",
      "                     E11.32       0.00      0.00      0.00         0\n",
      "                    E11.321       0.00      0.00      0.00         1\n",
      "                   E11.3211       1.00      1.00      1.00         1\n",
      "                   E11.3219       0.00      0.00      0.00         1\n",
      "                    E11.322       1.00      1.00      1.00         1\n",
      "                    E11.329       1.00      1.00      1.00         1\n",
      "                   E11.3299       0.00      0.00      0.00         1\n",
      "                   E11.3313       0.00      0.00      0.00         1\n",
      "                    E11.333       0.00      0.00      0.00         0\n",
      "                   E11.3392       0.00      0.00      0.00         0\n",
      "                   E11.3393       0.00      0.00      0.00         0\n",
      "                   E11.3411       0.00      0.00      0.00         0\n",
      "                   E11.3412       0.00      0.00      0.00         0\n",
      "                   E11.3491       0.00      0.00      0.00         1\n",
      "                   E11.3492       0.00      0.00      0.00         1\n",
      "                     E11.35       0.75      1.00      0.86         3\n",
      "                    E11.351       0.50      0.67      0.57         3\n",
      "                   E11.3521       0.00      0.00      0.00         1\n",
      "                   E11.3532       0.00      0.00      0.00         0\n",
      "                   E11.3533       0.00      0.00      0.00         2\n",
      "                   E11.3539       0.00      0.00      0.00         1\n",
      "                   E11.3541       0.00      0.00      0.00         2\n",
      "                   E11.3542       0.00      0.00      0.00         1\n",
      "                   E11.3543       0.00      0.00      0.00         1\n",
      "                   E11.3551       0.00      0.00      0.00         1\n",
      "                   E11.3553       0.00      0.00      0.00         0\n",
      "                    E11.359       0.67      0.80      0.73         5\n",
      "                   E11.3591       0.00      0.00      0.00         1\n",
      "                     E11.36       0.20      0.33      0.25         3\n",
      "                     E11.39       0.00      0.00      0.00         1\n",
      "                      E11.4       0.74      0.78      0.76        18\n",
      "                     E11.40       0.91      0.94      0.93        33\n",
      "              E11.40, G63.2       1.00      1.00      1.00         1\n",
      "                     E11.42       0.95      0.75      0.84        24\n",
      "                     E11.43       0.67      0.80      0.73        10\n",
      "             E11.43, K31.84       1.00      1.00      1.00         1\n",
      "                     E11.46       0.00      0.00      0.00         2\n",
      "                     E11.49       0.00      0.00      0.00         0\n",
      "                     E11.51       0.93      0.91      0.92        44\n",
      "                     E11.52       0.40      0.40      0.40         5\n",
      "                     E11.59       0.89      0.89      0.89         9\n",
      "                     E11.60       1.00      0.20      0.33         5\n",
      "                    E11.618       0.14      0.25      0.18         4\n",
      "                     E11.62       0.00      0.00      0.00         1\n",
      "                    E11.620       0.50      0.67      0.57         6\n",
      "                    E11.621       0.83      0.50      0.62        10\n",
      "           E11.621, L97.509       1.00      1.00      1.00         2\n",
      "                    E11.622       0.50      0.20      0.29         5\n",
      "                    E11.630       0.00      0.00      0.00         1\n",
      "                    E11.638       0.50      0.50      0.50         2\n",
      "                    E11.641       0.75      0.86      0.80         7\n",
      "                    E11.648       1.00      1.00      1.00         1\n",
      "                    E11.649       0.00      0.00      0.00         1\n",
      "                     E11.65       0.95      0.98      0.96        96\n",
      "                     E11.69       0.00      0.00      0.00         6\n",
      "                      E11.7       0.75      0.60      0.67         5\n",
      "                     E11.73       1.00      0.67      0.80         3\n",
      "                     E11.75       1.00      1.00      1.00         1\n",
      "                      E11.8       0.67      0.25      0.36         8\n",
      "                      E11.9       0.58      0.87      0.70        62\n",
      "E11.9, H35.89, I95.1, F41.9       0.00      0.00      0.00         1\n",
      "      E11.9, H35.89, R53.82       0.00      0.00      0.00         0\n",
      "              E11.9, I25.10       1.00      1.00      1.00         8\n",
      "               E11.9, I50.9       1.00      1.00      1.00        23\n",
      "             E11.9, I70.229       1.00      1.00      1.00         1\n",
      "                     E13.10       0.00      0.00      0.00         1\n",
      "                     E13.11       1.00      1.00      1.00         1\n",
      "                     E13.22       0.00      0.00      0.00         1\n",
      "                   E13.3291       0.00      0.00      0.00         0\n",
      "                   E13.3293       1.00      1.00      1.00         1\n",
      "                   E13.3313       0.00      0.00      0.00         1\n",
      "                   E13.3392       0.00      0.00      0.00         0\n",
      "                   E13.3411       0.00      0.00      0.00         0\n",
      "                   E13.3492       0.00      0.00      0.00         0\n",
      "                   E13.3511       0.50      1.00      0.67         1\n",
      "                   E13.3513       0.00      0.00      0.00         1\n",
      "                   E13.3522       0.00      0.00      0.00         1\n",
      "                   E13.3523       0.00      0.00      0.00         1\n",
      "                   E13.3529       0.00      0.00      0.00         1\n",
      "                   E13.3532       0.00      0.00      0.00         1\n",
      "                   E13.3542       0.00      0.00      0.00         0\n",
      "                   E13.3543       0.00      0.00      0.00         0\n",
      "                   E13.3551       0.00      0.00      0.00         0\n",
      "                   E13.37X2       0.00      0.00      0.00         1\n",
      "                     E13.40       0.00      0.00      0.00         0\n",
      "                     E13.44       0.00      0.00      0.00         1\n",
      "                    E13.610       0.00      0.00      0.00         0\n",
      "                    E13.622       0.00      0.00      0.00         0\n",
      "                     E13.65       0.00      0.00      0.00         2\n",
      "                      E28.2       1.00      1.00      1.00         1\n",
      "                      G56.0       0.00      0.00      0.00         1\n",
      "                      G60.9       0.00      0.00      0.00         1\n",
      "                      G90.3       0.00      0.00      0.00         0\n",
      "                      H28.0       0.00      0.00      0.00         0\n",
      "                    O24.011       0.00      0.00      0.00         1\n",
      "                    O24.012       0.00      0.00      0.00         1\n",
      "                    O24.013       0.00      0.00      0.00         0\n",
      "                     O24.02       0.00      0.00      0.00         0\n",
      "                    O24.112       0.00      0.00      0.00         1\n",
      "                    O24.119       0.00      0.00      0.00         0\n",
      "                     O24.13       0.00      0.00      0.00         1\n",
      "                    O24.311       0.00      0.00      0.00         1\n",
      "                     O24.33       0.00      0.00      0.00         1\n",
      "                      O24.4       0.60      0.75      0.67         4\n",
      "                    O24.410       0.96      0.96      0.96        45\n",
      "                    O24.414       0.00      0.00      0.00         0\n",
      "                    O24.419       1.00      0.71      0.83         7\n",
      "                    O24.429       0.00      0.00      0.00         1\n",
      "                     O24.43       0.67      1.00      0.80         2\n",
      "                    O24.435       0.00      0.00      0.00         1\n",
      "                    O24.439       0.00      0.00      0.00         0\n",
      "                    O24.812       0.00      0.00      0.00         0\n",
      "                    O24.813       0.00      0.00      0.00         1\n",
      "                    O24.819       0.00      0.00      0.00         0\n",
      "                      O24.9       0.00      0.00      0.00         1\n",
      "                     O99.21       0.00      0.00      0.00         2\n",
      "                     O99.22       0.00      0.00      0.00         1\n",
      "                      R73.0       1.00      0.83      0.91         6\n",
      "                     R73.02       1.00      1.00      1.00         6\n",
      "                     R73.03       1.00      1.00      1.00         1\n",
      "                     Z01.19       0.00      0.00      0.00         1\n",
      "                      Z34.1       0.00      0.00      0.00         0\n",
      "                     Z34.81       0.75      1.00      0.86         3\n",
      "                      Z39.2       0.00      0.00      0.00         1\n",
      "                      Z79.3       1.00      0.33      0.50         3\n",
      "                     Z83.41       1.00      1.00      1.00         2\n",
      "                     Z86.32       0.00      0.00      0.00         1\n",
      "\n",
      "                   accuracy                           0.72       900\n",
      "                  macro avg       0.26      0.25      0.25       900\n",
      "               weighted avg       0.74      0.72      0.72       900\n",
      "\n",
      "Neural network model, vectorizer, and label encoder saved successfully.\n",
      "Enter a patient note to predict ICD-10 code (or type 'exit' to quit): Patient complains of severe lower back pain that has persisted for several weeks. Pain radiates down the left leg. MRI indicates a herniated disc at the L4-L5 level.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted ICD-10 code: E11.7\n",
      "Enter a patient note to predict ICD-10 code (or type 'exit' to quit): Patient presents with uncontrolled diabetes mellitus. Blood sugar levels have been consistently high despite adherence to prescribed medication and dietary restrictions. Complains of frequent urination, increased thirst, and fatigue.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted ICD-10 code: E11.9\n",
      "Enter a patient note to predict ICD-10 code (or type 'exit' to quit): Patient reports chest pain radiating to the left arm and jaw, shortness of breath, and sweating. Symptoms began suddenly while exercising. History of hypertension and high cholesterol. ECG shows ST elevation\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted ICD-10 code: E11.51\n",
      "Enter a patient note to predict ICD-10 code (or type 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "# Ensure stopwords and wordnet are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Mock Data - Sheet1.csv')\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply data cleaning\n",
    "data['cleaned_notes'] = data['Patient Notes'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['encoded_labels'] = label_encoder.fit_transform(data['ICD-10 Code'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = data['cleaned_notes']\n",
    "y = data['encoded_labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform text data using TF-IDF with N-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()  # Convert to dense array\n",
    "X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Simple oversampling technique to balance the classes in the training set\n",
    "def oversample(X, y):\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    max_count = max(class_counts)\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in unique_classes:\n",
    "        X_cls = X[y == cls]\n",
    "        y_cls = y[y == cls]\n",
    "        n_samples = max_count - len(y_cls)\n",
    "        X_resampled.extend(X_cls)\n",
    "        y_resampled.extend(y_cls)\n",
    "        if n_samples > 0:\n",
    "            X_resampled.extend(X_cls[np.random.choice(len(X_cls), n_samples)])\n",
    "            y_resampled.extend([cls] * n_samples)\n",
    "    return np.array(X_resampled), np.array(y_resampled)\n",
    "\n",
    "X_train_tfidf, y_train = oversample(X_train_tfidf, y_train)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train_categorical = to_categorical(y_train, num_classes=len(np.unique(y)))\n",
    "y_test_categorical = to_categorical(y_test, num_classes=len(np.unique(y)))\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train_tfidf.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with epochs reduced to 4\n",
    "history = model.fit(X_train_tfidf, y_train_categorical, epochs=4, batch_size=32, validation_data=(X_test_tfidf, y_test_categorical))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_tfidf, y_test_categorical)\n",
    "print(f\"Neural Network Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict and calculate precision, recall, and F1-score\n",
    "y_pred_prob = model.predict(X_test_tfidf)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# Ensure the classification report includes all the classes present in y_test and y_pred\n",
    "unique_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "target_names = label_encoder.inverse_transform(unique_labels)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, labels=unique_labels, target_names=target_names, zero_division=0))\n",
    "\n",
    "# Save the model, vectorizer, and label encoder for future use\n",
    "model.save('icd10_nn_model.h5')\n",
    "with open('vectorizer.pkl', 'wb') as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "with open('label_encoder.pkl', 'wb') as le_file:\n",
    "    pickle.dump(label_encoder, le_file)\n",
    "\n",
    "print(\"Neural network model, vectorizer, and label encoder saved successfully.\")\n",
    "\n",
    "# Function to predict ICD-10 code for a new patient note\n",
    "def predict_icd10_nn(note):\n",
    "    cleaned_note = clean_text(note)\n",
    "    encoded_note = vectorizer.transform([cleaned_note]).toarray()\n",
    "    prediction_prob = model.predict(encoded_note)\n",
    "    prediction = np.argmax(prediction_prob, axis=1)\n",
    "    return label_encoder.inverse_transform([prediction[0]])[0]\n",
    "\n",
    "# Input for physicians or medical coders to check chart notes\n",
    "while True:\n",
    "    user_input = input(\"Enter a patient note to predict ICD-10 code (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    prediction = predict_icd10_nn(user_input)\n",
    "    print(f\"Predicted ICD-10 code: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
